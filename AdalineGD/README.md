# Adaline( 적응형 선정 뉴런 , ADaptive Linear NEuron)
- Perceptron의 향상된 버전
- 연속 함수로 비용을 정의하고 최소화 한다.
- 가중치를 업데이트하는 데 선형 활성화 함수를 사용한다. 최종 예측을 만드는 데에는 임계 함수 사용한다.
- 선형 활성화 함수 Φ(z)는 최종 입력과 동일한 함수이다. 즉, Φ(W^T x X) = W^T x X
- Perceptron 과의 차이점 :
    * Perceptron : true class label 과 predicted class label를 비교하여 오차를 계산.
    * Adaline : true class label 과 선형 활성화 함수의 출력 값 비교
        + Perceptron은 입력 -> 최종 입력함수 -> 임계함수 -> 출력, 오차 계산
        + Adaline은 입력 -> 최종 입력함수 -> 활성화 함수-> 임계함수, 오차 계산 -> 출력
- Adaline은 계산된 출력과 true class label사이의 제곱 오차 함으로 가중치를 학습할 비용 함수 J 정의한다.
    * J(W) = ½Σ(i)(y^(i) - Φ(z^(i)))^2
        + 단위 계단 함수 대신 연속적인 선형 활성화 함수를 사용하면 미분이 가능해진다.
        + 최적화 알고리즘인 경사 하강법을 적용하여 비용 함수를 최소화 하는 가중치를 찾을 수 있다.
    * 가중치 변화량 △Wj = -η∇J(W) = ηΣ(i)( y^(i) - Φ(z^(i)) ) x Xj  (∇J(W) : J(W)의 그래디언트(기울기))
        + Φ(z^(i)는 실수 이며 훈련 세트에 있는 모든 샘플을 기반으로 가중치 업데이트를 계산한다. ( 배치 경사 하강법 )
- Perceptron처럼 개별 훈련 샘플마다 평가한 후 업데이트 하는 것이 아니라 전체 훈련데이터 셋을 기반으로 기울기를 계산한다.
***
## 경사 하강법 (Gradiant Descent, GD)
- 함수의 기울기(경사)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜서 극값에 이를 때까지 반복시키는 것이다.
- 실제값과 예측값사이의 비용 함수( 평균 오차 제곱 )값이 최소가 되게 하는 기울기를 가진 일차 함수식을 찾기 위해 사용한다 ( 전역 최소값 ).
- 특성 스케일을 조절하여 경사 하강법 학습이 빠르게 수렴되도록 할 수 있다.
    + 표준화 특성 스케일 방법
        * 데이터에 표준 정규 분포의 성질을 부여하는 것.
        * j번째 특성 초기화 : xj' = ( xj - μj ) / σj 
***   
### 선형 회기  
- 선형적으로 분포되어 있는 데이터를 가장 잘 나타낼 수 있는 일차 함수를 찾는 것.
***
## 배치 경사 하강법
- 전체 훈련 세트에서 계산한 기울기의 반대 방향으로 진행하여 비용 함수를 최소화 하는 방법.
- 수백만 개의 데이터가 존재하는 매우 큰 데이터 셋이 존재하면 전역 최소값으로 나아가는 단계마다 매번 전체 훈련 데이터 셋을 다시 평가해야 하기 때문에 비용이 많이 든다.
- 대안
    1. 확률적 경사하강법(SGD)
        + 온라인 경사 하강법 이라고도 한다.
        + 모든 샘플에 대해 누적된 오차의 합을 기반으로 가중치를 업데이트 하는 것이 아닌 각 훈련 샘플에 조금씩 업데이트 한다.
        + 가중치가 자주 업데이트 되기 때문에 수렴 속도가 빠르다.
        + 기울이가 하나의 훈련 샘플을 기반으로 계산된다.
        + 반복이 충분하면 SGD가 효과는 있지만 노이즈가 매우 심하다.
        + '확률적(Stochastic)'이라는 용어는 무작위로 선택된다는 것을 나타낸다.
    2. 미니 배치 확률적 경사하강법(mini batch SGD)
        + 일반적으로 무작위로 선택한 10개에서 1,000개 사이의 예로 구성된다.
        + 미니 배치 SGD는 SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적이다.
